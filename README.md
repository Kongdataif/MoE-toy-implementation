PyTorch Implementation of Adaptive Mixtures of Local Experts (MoE)"If you have a big problem, divide it into smaller ones and let experts solve them."This repository contains a PyTorch implementation of the classic paper "Adaptive Mixtures of Local Experts" (Jacobs et al., 1991). It demonstrates how a neural network can learn to decompose complex tasks into simpler sub-tasks using a competitive learning mechanism.üìå IntroductionUnlike traditional ensemble methods that average the outputs of all models, Mixture of Experts (MoE) uses a Gating Network to dynamically select or weight the most competent expert for a given input region.+1This project implements the MoE architecture from scratch and verifies its Task Decomposition capability using non-linear regression tasks (V-Shape and W-Shape data).+2üß† Theory & Methodology1. ArchitectureThe model consists of two main components sharing the same input :Expert Networks ($E_i$): $N$ independent neural networks that specialize in different regions of the input space.Gating Network ($G$): A network that outputs a probability distribution (via Softmax) over the experts, deciding which expert to trust.2. Competitive Learning (The Loss Function)To encourage specialization (competition), we use the Negative Log-Likelihood (NLL) of a Gaussian Mixture Model instead of simple MSE.$$E = - \log \sum_{i} p_i e^{-\frac{1}{2} || \mathbf{d} - \mathbf{o}_i ||^2}$$$\mathbf{d}$: Target (Ground Truth)$\mathbf{o}_i$: Output of Expert $i$$p_i$: Probability assigned by the Gating Network3. Gradient & Winner-Take-AllThe gradient of this loss function includes a Posterior Probability term:$$\frac{\partial E}{\partial \mathbf{o}_i} = - \left[ \frac{p_i e^{-\frac{1}{2}||\dots||}}{\sum_j p_j e^{-\frac{1}{2}||\dots||}} \right] (\mathbf{d} - \mathbf{o}_i)$$Mechanism: If an expert performs well (small error), its posterior probability increases.Effect: The gradient update is weighted by this probability. The "winner" receives a strong learning signal, while "losers" receive almost zero gradient, leading to efficient task decomposition .üõ†Ô∏è Implementation DetailsThe core logic lies in the custom loss function that implements the competitive dynamics.Pythonclass MoELoss(nn.Module):
    def forward(self, expert_outputs, gate_probs, target):
        # 1. Squared Error for each expert
        squared_error = (target.unsqueeze(1) - expert_outputs).pow(2).sum(dim=2)
        
        # 2. Convert to Gaussian Likelihood
        likelihood = torch.exp(-0.5 * squared_error)
        
        # 3. Weighted Sum by Gating Probabilities
        weighted_likelihood = (gate_probs * likelihood).sum(dim=1)
        
        # 4. Negative Log-Likelihood
        loss = -torch.log(weighted_likelihood + 1e-8)
        return loss.mean()
üß™ Experiments & ResultsCase 1: V-Shape Data (2 Experts)Task: Fit a V-shaped function ($y=|x| + \text{noise}$).Configuration: 2 Linear Experts.Result: The model perfectly splits the task. One expert handles the left slope ($x<0$), and the other handles the right slope ($x>0$).(Fig 1. Red and Blue dots represent data points assigned to Expert A and Expert B, respectively.) Case 2: W-Shape Data (4 Experts)Task: Fit a W-shaped function (Double V).Configuration: 4 Experts.Result: The experts automatically divide the domain into 4 distinct linear segments.(Fig 2. Four experts (Red, Blue, Green, Orange) successfully decomposing the W-shape function.) +1üìà Performance Tuning GuideBased on experiments, here are strategies to improve MoE performance:PriorityStrategyMechanismImplementation1Increase CapacityAllow experts to learn non-linear patterns (Curves vs Lines).nn.Linear $\to$ nn.Sequential(Linear, ReLU, Linear)2Optimizer ScheduleReduce learning rate near convergence for fine-tuning.torch.optim.lr_scheduler.StepLR3Scale UpIncrease num_experts for highly complex data (e.g., "WW" shape).MoE(num_experts=8)4Learn VarianceAllow experts to output confidence ($\sigma$), sharpening the competition.Custom Gaussian PDF LossüöÄ How to RunClone the repository:Bashgit clone https://github.com/YOUR_USERNAME/MoE-PyTorch-Implementation.git
Install dependencies:Bashpip install torch matplotlib numpy
Run the training script:Bashpython train.py
üìö ReferencesPaper: Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive Mixtures of Local Experts. Neural Computation.Course: Graduate School of AI, Sogang University.
